{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pyrootutils\n",
    "\n",
    "root = pyrootutils.setup_root(\n",
    "    search_from=os.getcwd(),\n",
    "    indicator=\".project-root\",\n",
    "    pythonpath=True,\n",
    "    dotenv=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import hydra\n",
    "import omegaconf\n",
    "\n",
    "import src.eval\n",
    "import src.utils\n",
    "import src.utils.plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# to use ensemble: i) set multiple model_dirs, ii) set to parent directory of several model_dirs, iii) set to glob pattern (e.g. logs/train/multiruns/2023_10-14_*)\n",
    "model_dir = [\n",
    "    \"logs/train/multiruns/2023-10-13_15-25-56/0\",\n",
    "    \"logs/train/multiruns/2023-10-13_15-25-56/1\",\n",
    "]\n",
    "model_dir = [src.utils.get_absolute_project_path(md) for md in model_dir]\n",
    "\n",
    "config_path = os.path.join(\n",
    "    \"..\", \"..\", \"configs\", \"eval.yaml\"\n",
    ")  # NB: relative to <project_root>/src/utils (must be relative path)\n",
    "\n",
    "config_overrides_dot = [  # same notation as for cli overrides (dot notation). Useful for changing whole modules, e.g. change which datamodule file is loaded\n",
    "    \"++extras.disable_pytorch_lightning_output=True\",\n",
    "    \"++eval.kwargs.show_warnings=False\",\n",
    "]\n",
    "config_overrides_dict = dict(\n",
    "    model_dir=model_dir\n",
    ")  # Dictionary with overrides. Useful for larger changes/additions/deletions that does not exist as entire files.\n",
    "\n",
    "cfg = src.utils.initialize_hydra(\n",
    "    config_path,\n",
    "    config_overrides_dot,\n",
    "    config_overrides_dict,\n",
    "    return_hydra_config=True,\n",
    "    print_config=False,\n",
    ")  # print config to inspect if all settings are as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "object_dict = src.utils.initialize_saved_objects(cfg)\n",
    "model, datamodule, trainer, logger = (\n",
    "    object_dict[\"model\"],\n",
    "    object_dict[\"datamodule\"],\n",
    "    object_dict.get(\"trainer\"),\n",
    "    object_dict.get(\"logger\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with omegaconf.open_dict(cfg):\n",
    "    cfg.eval.kwargs.forecast_horizon = 6\n",
    "    cfg.eval.kwargs.stride = 6\n",
    "    cfg.eval.plot.every_n_prediction = 1\n",
    "    cfg.eval.plot.presenter = [\n",
    "        \"show\",\n",
    "        \"savefig\",\n",
    "    ]  # set presenter to \"show\" to show figures in output, and \"savefig\" to save them to the model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluate\n",
    "The src.eval.run function returns the configured metrics over the evaluated split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metric_dict, eval_object_dict = src.eval.run(cfg, datamodule, model, trainer, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare model with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_baseline = cfg.copy()\n",
    "with omegaconf.open_dict(cfg_baseline):\n",
    "    del cfg_baseline.model\n",
    "    del cfg_baseline.model_dir\n",
    "    if omegaconf.OmegaConf.select(cfg_baseline, \"eval.kwargs.retrain\") is not None:\n",
    "        cfg_baseline.eval.kwargs.retrain = True\n",
    "\n",
    "cfg_baseline = src.utils.initialize_hydra(\n",
    "    config_path,\n",
    "    [\"model=baseline_naive_seasonal\"],\n",
    "    cfg_baseline,\n",
    "    return_hydra_config=False,\n",
    "    print_config=False,\n",
    ")  # print config to inspect if all settings are as expected\n",
    "\n",
    "baseline_model = hydra.utils.instantiate(cfg_baseline.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with omegaconf.open_dict(cfg):\n",
    "    cfg.eval.plot = False\n",
    "    cfg.eval.predictions = {\"return\": {\"data\": True}}\n",
    "\n",
    "with omegaconf.open_dict(cfg_baseline):\n",
    "    cfg_baseline.eval.plot = False\n",
    "    cfg_baseline.eval.predictions = {\"return\": True}\n",
    "\n",
    "metric_dict, eval_object_dict = src.eval.run(cfg, datamodule, model, trainer, logger)\n",
    "baseline_metric_dict, baseline_eval_object_dict = src.eval.run(\n",
    "    cfg_baseline, datamodule, baseline_model, trainer, logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_string(metrics):\n",
    "    return \" \".join([f\"{'_'.join(k.split('_')[1:])}={v:.2E}\" for k, v in metrics.items()])\n",
    "\n",
    "\n",
    "fig = src.utils.plotting.plot_prediction(\n",
    "    eval_object_dict[\"predictions\"],\n",
    "    eval_object_dict[\"predictions_data\"],\n",
    "    model,\n",
    "    None,\n",
    "    separate_target=False,\n",
    "    plot_covariates=False,\n",
    "    plot_encodings=False,\n",
    "    plot_past=False,\n",
    "    plot_prediction_point=False,\n",
    "    fig_title=f\"Model: {metric_string(metric_dict)}; Baseline: {metric_string(baseline_metric_dict)}\",\n",
    ")\n",
    "_ = baseline_eval_object_dict[\"predictions\"].plot(label=\"baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
