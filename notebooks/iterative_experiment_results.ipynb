{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyrootutils\n",
    "\n",
    "root = pyrootutils.setup_root(\n",
    "    search_from=os.getcwd(),\n",
    "    indicator=[\".git\", \"pyproject.toml\"],\n",
    "    pythonpath=True,\n",
    "    dotenv=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm\n",
    "import glob\n",
    "import ast\n",
    "import re\n",
    "import shutil\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import mlflow\n",
    "import matplotlib\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Iterative Experiment Results\n",
    "This notebook aids in inspecting the results of the iterative data chunks experiments, and in collecting the necessary information for the paper: gathering the figures and outputting latex table data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(row):\n",
    "    if row[\"tags.ensemble\"] == \"True\":\n",
    "        return \"ReWTS\"\n",
    "    if row[\"params.experiment/scale_model_parameters\"] == \"True\":# and row[\"start_time\"].year >= 2024:  #TODO: temp fix for xgb\n",
    "        return \"Global\"\n",
    "    else:\n",
    "        return \"Global No Scaling\"\n",
    "\n",
    "\n",
    "def search_mlflow(search_experiment_name, mlflow_dir=os.path.join(root, \"logs\", \"mlflow\", \"mlruns\")):\n",
    "    tags_model_to_name = dict(XGB=\"XGBoost\", TCN=\"TCN\", RNN=\"LSTM\", Regression=\"ElasticNet\")\n",
    "    if isinstance(search_experiment_name, str):\n",
    "        search_experiment_name = [search_experiment_name]\n",
    "    mlflow.set_tracking_uri(f\"file://{mlflow_dir}\")\n",
    "    df = mlflow.search_runs(experiment_names=search_experiment_name)\n",
    "    df['model_name'] = df.apply(get_model_name, axis=1)\n",
    "    df['tags.model'] = df['tags.model'].apply(lambda x: tags_model_to_name[x.replace(\"Model\", \"\")])\n",
    "\n",
    "    return df\n",
    "    \n",
    "\n",
    "def set_matplotlib_attributes(font_size=8, font=\"DejaVu Sans\"):\n",
    "    matplotlib.rcParams.update({'font.size': font_size, 'font.family': font})\n",
    "\n",
    "def set_figure_size(fig, column_span, height=None):\n",
    "    if height is None:\n",
    "        height = 5 if column_span == \"double\" else 10\n",
    "    \n",
    "    cm = 1 / 2.54\n",
    "    if column_span == \"single\":\n",
    "        fig_width = 8.4 * cm\n",
    "    elif column_span == \"double\":\n",
    "        fig_width = 17.4 * cm\n",
    "    elif isinstance(column_span, (int, float)):\n",
    "        fig_width = column_span\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    figsize = (fig_width, height * cm)\n",
    "\n",
    "    fig.set_size_inches(*figsize)\n",
    "\n",
    "def save_figure(fig, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    fig.savefig(path + \".pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "    fig.savefig(path + \".png\", format=\"png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_column_span = 10 #\"double\"\n",
    "fig_height = 10\n",
    "set_matplotlib_attributes(font_size=10)\n",
    "\n",
    "tags_model_to_name = dict(XGB=\"XGBoost\", TCN=\"TCN\", RNN=\"LSTM\", Regression=\"ElasticNet\")\n",
    "# Column names\n",
    "chunk_idx_column = \"params.datamodule/chunk_idx\"\n",
    "model_name_column = \"model_name\"\n",
    "chunk_idx_plot_name = \"Chunk #\"\n",
    "\n",
    "metric_name = \"test_mse\"\n",
    "metric_plot_name = \" \".join(metric_name.replace(\"test_\", \"\").split(\"_\")).upper()\n",
    "metric_column = f\"metrics.{metric_name}\"\n",
    "\n",
    "models = [\"elastic-net-hopt\"] #[\"xgboost\", \"elastic_net\", \"tcn\", \"rnn\"]\n",
    "dataset = \"electricity\"\n",
    "\n",
    "broken = False\n",
    "broken_limit = 100\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    search_experiment_name = f\"{dataset}_eval-it_{model}\"\n",
    "    search_experiment_name = [search_experiment_name, search_experiment_name + \"-no-scale\"]\n",
    "    df = search_mlflow(search_experiment_name)\n",
    "\n",
    "    if \"Global No Scaling\" in df[\"model_name\"].values:\n",
    "        model_order = [\"ReWTS\", \"Global\", \"Global No Scaling\"]\n",
    "    else:\n",
    "        model_order = [\"ReWTS\", \"Global\"]\n",
    "    \n",
    "    # Group DataFrame by 'chunk_idx' and 'model_name' and get the mean of the metric column\n",
    "    grouped = df.groupby([chunk_idx_column, model_name_column])[metric_column].mean().reset_index()\n",
    "    \n",
    "    # Rename columns for better plotting\n",
    "    grouped = grouped.rename(columns={metric_column: metric_plot_name, chunk_idx_column: chunk_idx_plot_name})\n",
    "    \n",
    "    # Sort by 'chunk_idx' numerically\n",
    "    grouped[chunk_idx_plot_name] = grouped[chunk_idx_plot_name].astype(int)\n",
    "    grouped[chunk_idx_plot_name] += 1\n",
    "    #grouped = grouped[grouped[chunk_idx_plot_name] <= chunk_end]\n",
    "    grouped = grouped.sort_values(by=[chunk_idx_plot_name, model_name_column])\n",
    "\n",
    "    if broken:\n",
    "        fig, (ax_higher, ax_lower) = plt.subplots(ncols=1, nrows=2, sharex=True)\n",
    "        broken_lower = grouped.copy()\n",
    "        broken_higher = grouped.copy()\n",
    "    \n",
    "        # Update the metric_plot_name column where the condition is met\n",
    "        broken_lower.loc[broken_lower[metric_plot_name] >= broken_limit, metric_plot_name] = broken_limit\n",
    "        broken_higher.loc[broken_higher[metric_plot_name] < broken_limit, metric_plot_name] = 0\n",
    "    \n",
    "        # Plotting\n",
    "        catplot = sns.barplot(data=broken_lower, x=chunk_idx_plot_name, y=metric_plot_name, hue=model_name_column, ax=ax_lower)\n",
    "        catplot = sns.barplot(data=broken_higher, x=chunk_idx_plot_name, y=metric_plot_name, hue=model_name_column, ax=ax_higher)\n",
    "\n",
    "        ax_higher.set_ylim(bottom=broken_limit)\n",
    "        ax_lower.set_ylim(0, broken_limit)\n",
    "\n",
    "        set_figure_size(fig, fig_column_span)\n",
    "    \n",
    "        # Get current x-axis tick labels\n",
    "        xticks = ax_lower.get_xticklabels()\n",
    "        \n",
    "        # Set every other tick label to be empty\n",
    "        new_xticks = [xticks[i] if i % 2 == 0 else '' for i in range(len(xticks))]\n",
    "        ax_lower.set_xticklabels(new_xticks)#, rotation=45)\n",
    "\n",
    "        # the upper part does not need its own x axis as it shares one with the lower part\n",
    "        ax_higher.get_xaxis().set_visible(False)\n",
    "\n",
    "        higher_yticks = ax_higher.get_yticks()\n",
    "        higher_yticks[0] = broken_limit\n",
    "        ax_higher.set_yticks(higher_yticks)\n",
    "        ax_higher.set_yticklabels(higher_yticks)\n",
    "        \n",
    "        # by default, each part will get its own \"Latency in ms\" label, but we want to set a common for the whole figure\n",
    "        # first, remove the y label for both subplots\n",
    "        ax_lower.set_ylabel(\"\")\n",
    "        ax_higher.set_ylabel(\"\")\n",
    "\n",
    "        #ax_higher.grid(visible=True, axes=\"y\"\n",
    "        # then, set a new label on the plot (basically just a piece of text) and move it to where it makes sense (requires trial and error)\n",
    "        fig.text(0.05, 0.55, metric_plot_name, va=\"center\", rotation=\"vertical\")\n",
    "    \n",
    "        #catplot.set(ylim=(0,65))\n",
    "    \n",
    "        # Remove the legend title\n",
    "        #new_legend = catplot.get_figure().get_legend()\n",
    "        #new_legend.set_title('')\n",
    "        # Set the legend inside the plot in the top left corner\n",
    "        #new_legend.set_bbox_to_anchor((0.32, 0.90), transform=catplot.ax.transAxes) \n",
    "        # Remove the legend title\n",
    "        legend = catplot.legend_\n",
    "        legend.set_title('')\n",
    "        legend.set_frame_on(False)\n",
    "        ax_lower.get_legend().remove()\n",
    "    else:\n",
    "        # Plotting\n",
    "        catplot = sns.catplot(data=grouped, x=chunk_idx_plot_name, y=metric_plot_name, hue=model_name_column, kind=\"bar\", hue_order=model_order)\n",
    "        fig = catplot.fig\n",
    "        set_figure_size(fig, fig_column_span, height=fig_height)\n",
    "    \n",
    "        # Get current x-axis tick labels\n",
    "        xticks = catplot.ax.get_xticklabels()\n",
    "        \n",
    "        # Set every other tick label to be empty\n",
    "        new_xticks = [xticks[i] if i % 2 == 0 else '' for i in range(len(xticks))]\n",
    "        catplot.set_xticklabels(new_xticks)#, rotation=45)\n",
    "    \n",
    "        #catplot.set(ylim=(0,65))\n",
    "    \n",
    "        # Remove the legend title\n",
    "        new_legend = catplot._legend\n",
    "    \n",
    "        # Set the legend inside the plot in the top left corner\n",
    "        new_legend.set_bbox_to_anchor((0.32, 0.90), transform=catplot.ax.transAxes)         \n",
    "        new_legend.set_title('')\n",
    "\n",
    "    # Set title\n",
    "    plot_title = df[\"tags.model\"][0]\n",
    "    catplot.set(title=plot_title)\n",
    "\n",
    "    fig_folder_name = search_experiment_name if isinstance(search_experiment_name, str) else search_experiment_name[0]\n",
    "\n",
    "\n",
    "    fig_folder = os.path.join(root, \"figures\", fig_folder_name)\n",
    "    save_figure(fig, os.path.join(fig_folder, f\"chunk_{metric_name}_iterative_{dataset}_{model}\"))\n",
    "    grouped.to_csv(os.path.join(fig_folder, \"iterative_dataframe.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_decimals = 2\n",
    "tags_model_to_name = dict(XGB=\"XGBoost\", TCN=\"TCN\", RNN=\"LSTM\", Regression=\"ElasticNet\")\n",
    "print(df[\"tags.model\"][0].replace(\"Model\", \"\"))\n",
    "#print(tags_model_to_name[df[\"tags.model\"][0].replace(\"Model\", \"\")])\n",
    "mean_values = grouped.groupby(\"model_name\")[metric_plot_name].mean()\n",
    "# Properly set the float format option using a lambda function\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.{desired_decimals}e}')\n",
    "mean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_difference(value1, value2):\n",
    "    return (value1 - value2) / ((value1 + value2) / 2) * 100\n",
    "\n",
    "model_names = mean_values.index.unique()\n",
    "combinations = itertools.combinations(model_names, 2)\n",
    "\n",
    "for combo in combinations:\n",
    "    percent_diff = percentage_difference(mean_values.loc[combo[0]], mean_values.loc[combo[1]])\n",
    "    print(f\"Percentage difference for {df['tags.model'][0].replace('Model', '')} between {combo[0]} and {combo[1]}: {percent_diff:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_end = 46\n",
    "fig_column_span = \"double\"\n",
    "set_matplotlib_attributes()\n",
    "\n",
    "dataset = \"electricity\"\n",
    "stages = [\"train\", \"eval\"]\n",
    "stages_plot_name = [\"Training\", \"Inference\"]\n",
    "\n",
    "tags_model_to_name = dict(XGB=\"XGBoost\", TCN=\"TCN\", RNN=\"LSTM\", Regression=\"ElasticNet\")\n",
    "\n",
    "# Column names\n",
    "chunk_idx_column = \"params.experiment/chunk_idx\"\n",
    "model_name_column = \"model_name\"\n",
    "chunk_idx_plot_name = \"Chunk #\"\n",
    "\n",
    "\n",
    "for stage_i, stage in enumerate(stages):\n",
    "    if stage == \"eval\":\n",
    "        search_experiment_name = [f\"{dataset}-eval_it-xgb-eval_time\"]\n",
    "    else:\n",
    "        search_experiment_name = [f\"{dataset}-train_time-xgboost-full\", f\"{dataset}-train_time-xgboost-ensemble\"]\n",
    "    df = search_mlflow(search_experiment_name)\n",
    "    \n",
    "    metric_name = f\"{stage}_execution_time\"\n",
    "    metric_plot_name = f\"{stages_plot_name[stage_i]} execution time (s)\"\n",
    "    metric_column = f\"metrics.{metric_name}\"\n",
    "\n",
    "    if stage == \"Inference\":\n",
    "        chunk_idx_column = \"params.datamodule/chunk_idx\"\n",
    "\n",
    "\n",
    "    # Group DataFrame by 'chunk_idx' and 'model_name' and get the mean of the metric column\n",
    "    grouped = df.groupby([chunk_idx_column, model_name_column])[metric_column].mean().reset_index()\n",
    "    \n",
    "    # Rename columns for better plotting\n",
    "    grouped = grouped.rename(columns={metric_column: metric_plot_name, chunk_idx_column: chunk_idx_plot_name})\n",
    "    \n",
    "    # Sort by 'chunk_idx' numerically\n",
    "    grouped[chunk_idx_plot_name] = grouped[chunk_idx_plot_name].astype(int)\n",
    "    grouped[chunk_idx_plot_name] += 1\n",
    "    grouped = grouped[grouped[chunk_idx_plot_name] <= chunk_end]\n",
    "    grouped = grouped.sort_values(by=[chunk_idx_plot_name, model_name_column])\n",
    "    \n",
    "    if stage == \"train\":\n",
    "        # Accumulate the metric\n",
    "        grouped[metric_plot_name] = grouped.groupby(model_name_column)[metric_plot_name].cumsum()\n",
    "    \n",
    "    # Plotting\n",
    "    catplot = sns.catplot(data=grouped, x=chunk_idx_plot_name, y=metric_plot_name, hue=model_name_column, kind=\"bar\")\n",
    "    set_figure_size(catplot.fig, column_span=fig_column_span)\n",
    "    \n",
    "    plt.yscale('log')\n",
    "    # Rotate x-axis labels for better readability\n",
    "    catplot.set_xticklabels(rotation=45)\n",
    "    \n",
    "    #catplot.set(ylim=(0,65))\n",
    "    \n",
    "    # Remove the legend title\n",
    "    new_legend = catplot._legend\n",
    "    new_legend.set_title('')\n",
    "    # Set the legend inside the plot in the top left corner\n",
    "    new_legend.set_bbox_to_anchor((0.25, 0.95), transform=catplot.ax.transAxes) \n",
    "    \n",
    "    # Set title\n",
    "    plot_title = df[\"tags.model\"][0]\n",
    "    catplot.set(title=plot_title)\n",
    "    \n",
    "    fig_folder_name = f\"execution_time_{dataset}\"\n",
    "    fig_path = os.path.join(root, \"figures\", fig_folder_name, f\"execution_time_{stages_plot_name[stage_i]}\")\n",
    "    save_figure(catplot.figure, fig_path)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
