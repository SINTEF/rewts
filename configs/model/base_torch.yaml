# @package _global_

defaults:
  - /logger: tb_mlflow.yaml

model:
  _target_: ???

  model_name: null # TODO: not relevant?
  input_chunk_length: 10
  output_chunk_length: 5
  loss_fn:
    _target_: torch.nn.MSELoss
  batch_size: 128
  n_epochs: 10
  #work_dir, not used when trainer is used?
  #log_tensorboard: False
  add_encoders: null # TODO
  random_state: null # TODO: (is this made obsolete by torch.seed_everything?)
  show_warnings: True # TODO: for debug
  #torch_metrics:  # You can add additional metrics to be logged
  #  _target_: torchmetrics.SymmetricMeanAbsolutePercentageError
  optimizer_cls:
    _target_: torch.optim.Adam
    _partial_: True
  optimizer_kwargs:
    lr: 1e-3
  lr_scheduler_cls: null # TODO
  lr_scheduler_kwargs: null

eval:
  kwargs:
    retrain: False # By default, models are retrained (causually, i.e. when predicting sample 100 it is trained on samples 1-99) on dataset when evaluated (val/test).
